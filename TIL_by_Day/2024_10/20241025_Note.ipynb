{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024년 10월 25일 금요일\n",
    "## 머신러닝/딥러닝 주차 과제 노트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 과제를 해결하면서 공부한 부분을 정리\n",
    "#### 목적은 각 부분에서, 이 과정을 왜 하는지를 알았지만 정리를 통해 더 정확하게 짚고 넘어가고싶다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터 셋 불러오기\n",
    "import seaborn as sns\n",
    "titanic = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 데이터 셋 불러오기\n",
    "seaborn은 **데이터 시각화**를 위한 라이브러리이다.\n",
    "이 라이브러리를 만든 Michael Waskom이 해양과 관련된 테마에서 영감을 받아 지은 이름이다. 구체적으로 seaborn이라는 단어 자체는 '바다(born of the sea)'에서 유래한 것으로, 데이터 시각화에서 데이터를 이해하고 해석하는 과정을 바다를 항해하면서 지도를 그리는 것에 비유한 의미가 담겨있다.\n",
    "\n",
    "sns라는 약어는 seaborn 라이브러리를 불러올 때 자주 사용되는 관습적인 약어이다.\n",
    "\n",
    "seaborn에는 사용자가 데이터 시각화 기법을 연습할 수 있도록 다양한 샘플 데이터 셋을 미리 제공하며, 이 데이터셋은 load_dataset() 함수를 통해 쉽게 불러올 수 있다. titanic 외에도 iris, tips, penguins 등 여러 유명한 샘플 데이터 셋이 포함되어 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. feature 분석\n",
    "\n",
    "feature는 말 그대로 데이터의 특징 또는 변수를 의미한다. 머신러닝과 데이터 분석에서 feature는 모델이 학습할 수 있는 입력 데이터의 속성 들을 말한다. 예를 들어, 타이타닉 데이터셋에서 승객의 나이, 성별, 좌석 클래스 등은 각각 하나의 feature가 된다.\n",
    "\n",
    "**1. 데이터 프레임의 첫 5행을 확인 (head 함수)**\n",
    "- **목적:** 데이터셋의 전반적인 구조와 각 열(column)이 무엇을 나타내는 지 파악한다. 이를 통해 데이터셋에 어떤 종류의 feature가 포함되어 있는 지 확인하고, 데이터의 첫 인상을 형성한다.\n",
    "- **핵심:** 각 열이 어떤 정보(변수)를 나타내는 지 확인하고, 분석에 유용한 변수들을 식별한다.\n",
    "\n",
    "**2. 기본적인 통계 확인 (describe 함수)**\n",
    "- **목적:** 각 열의 기본 통계값을 계산하여 데이터의 분포를 이해하고, 극단적인 값이나 데이터의 경향성을 파악한다. \n",
    "- **핵심 설명:** \n",
    "    - count: 각 열의 유효한 데이터 갯수를 의미함. 이 값이 작으면 해당 열에 결측치가 많다는 것을 알 수 있다.\n",
    "    - mean: 각 열의 평균값으로, 데이터의 중심 경향을 나타낸다.\n",
    "    - std(표준편차): 데이터의 흩어짐을 나타내며, 값이 클수록 데이터가 넓게 퍼져 있음을 나타낸다.\n",
    "    - min: 각 열의 최소값\n",
    "    - 25%: 1사분위수로, 데이터의 하위 25%에 해당하는 값을 의미한다.\n",
    "    - 50%: 중앙값(2사분위수)으로, 데이터의 중간값을 확인한다.\n",
    "    - 75%: 3사분위수로, 데이터의 상위 25%에 해당하는 값을 의미한다.\n",
    "    - max: 각 열이 최댓값이다. \n",
    "\n",
    "- **핵심:** 각 feature의 분포를 확인하고, 이상치의 데이터의 특성을 파악하는 데 도움을 준다.\n",
    "\n",
    "**3. 결측치 확인**\n",
    "- **목적:** 각 열에 존재하는 결측치의 개수를 파악한다. 결측치가 많은 열은 분석 과정에서 처리해야 할 필요가 있다.\n",
    "- **핵심:** 결측치가 많은 열이나 분석에 중요한 열의 결측치를 식별하여, 데이터 전처리 전략을 세울 수 있다.\n",
    "\n",
    "결론적으로, 이러한 단계들은 데이터셋을 깊이 이해하고, 분석 과정에서 중요한 feature들을 식별하며, 데이터의 품질을 파악하며 향후 모델링에 유리한 방향을 결정하는 데 중요한 역할을 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. feature engineering\n",
    "3-1. 결측치 처리\n",
    "Age(나이)의 결측치는 중앙값으로, Embarked(승선 항구)의 결측치는 최빈값으로 대체해주세요. 모두 대체한 후에, 대체 결과를 isnull() 함수와 sum()  함수를 이용해서 확인해주세요. \n",
    "- **작업:** 결측치를 중앙값(Age)와 최빈값(Embarked)으로 대체\n",
    "- **이유:** 결측치는 모델의 성능을 떨어트리고 오류를 유발한다. age의 경우 중앙값으로 대체하는 이유는 분포가 크게 치우치지 않도록 하기 위함이고, Embarked는 범주형 데이터이므로 가장 많이 등장한 값으로는 대체하는 것이 합리적이다. 이렇게 하면 데이터의 일관성를 유지하면서, 모델링 과정에서 결측치로 인한 오류를 방지할 수 있다. \n",
    "\n",
    "#### 결측치 처리에서 많이 사용하는 값 \n",
    "(1) 최소값을 사용할 수 있는 경우\n",
    "- 비용/비율 데이터: 예를 들어, 데이터가 제품의 원가나 최소한의 비용 등을 나타내는 경우, 결측치를 최소값으로 대체하는 게 의미가 있을 수 있다.\n",
    "- 로그 변환된 데이터: 로그 변환된 데이터의 경우, 결측치를 처리할 때 종종 최소값 또는 1과 같은 작은 값을 선택할 수 있다. 로그 변환 후 0이나 음수의 값이 의미가 없기 때문이다.\n",
    "(2) 결측치 처리에서 많이 사용하는 값\n",
    "- 최대값: 최대값은 결측치 처리를 위해 거의 사용되지 않지만, 상황에 따라 결측치가 \"최대 허용값\" 또는 특정 한계값일 때 사용할 수 있다.\n",
    "- 중앙값(Median): 데이터에 극단값(outlier)이 있을 때, 중앙값이 평균값보다 결측치 처리에 더 효과적이다.\n",
    "- 최빈값(Mode): 범주형 데이터 또는 특정 값이 자주 반복되는 데이터에서 사용한다.\n",
    "- 평균값(Mean): 데이터가 정규분포에 가깝고, 극단값의 영향을 적게 받을 때 사용한다.\n",
    "(3) 결측치 처리 전략 선택 기준\n",
    "결측치를 처리할 때는 데이터의 특성과 목적을 고려하는 것이 중요하다. 처리 방법을 선택할 때 고려해야 할 요소들은 다음과 같다. \n",
    "- 데이터의 분포: 데이터가 정규분포를 따르는지, 극단값이 있는지 등을 파악하여 평균, 중앙값을 선택한다.\n",
    "- 변수의 의미: 변수의 성격에 따라 값의 선택을 다르게 할 수 있다. 예를 들어, 나이는 최소값이나 최대값으로 대체하지 않는 것이 일반적이다.\n",
    "- 분석 목적: 결측치를 어떻게 처리하느냐에 따라 분석 결과가 크게 달라질 수 있으므로, 분석 목적에 맞게 적절한 처리를 선택해야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-2. 수치형으로 인코딩\n",
    "- **작업:** 성별과 생존 여부, 승선 항구 정보를 수치형 데이터로 변환하였다.\n",
    "- **이유:** 머신러닝 모델은 숫자를 입력으로 받아야 하기 때문에, 범주형 데이터를 수치형으로 변환해야한다. 이 작업을 통해 데이터셋을 모델에 입력할 수 있는 형식으로 준비하고, 모델이 범주형 변수의 정보를 학습할 수 있도록 합니다. \n",
    "\n",
    ".map() 함수는 pandas series 객체에서 제공되는 함수로, 각 요소에 대해 특정 함수나 매핑 규칙을 적용할 때 사용된다. 주로 데이터 변환이나 레이블 인코딩 등을 할 때 유용하게 사용된다.\n",
    "\n",
    ".map()의 활용 방법\n",
    "1. 딕셔너리로 값 매핑\n",
    "주어진 series의 값들을 딕셔너리에 정의된 매칭 규칙에 따라 변환한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 각 함수로 매핑\n",
    "각 요소에 대해 지정한 함수를 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic['age_group'] = titanic['age'].map(lambda x: 'adult' if x >= 18 else 'child')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드는 titanic['age'] 열의 값을 기준으로, 18세 이상이면 adult, 그 미만이면 child로 변환한 새로운 age_group 열을 생성한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3-3. 새로운 feature 생성\n",
    "- **작업:** SibSp와 Parch를 합하여 family_size라는 새로운 feature를 생성한다.\n",
    "- **이유:** 새로운 feature를 생성함으로써 모델이 승객의 각족 크기라는 중요한 정보를 학습할 수 있게 된다. 이런 추가적인 정보는 모델의 성능을 향상시킬 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 모델 학습시키기\n",
    "\n",
    "4-1. 학습 데이터 필터링\n",
    "- **작업:** 학습에 필요한 feature를 선택하고, target 변수를 분리한 후 스케일링을 진행하였다.\n",
    "- **이유:** 데이터 스케일링은 모델 학습에서 중요한 단계이다. 특히, Logistic Regression과 XGBoost와 같은 알고리즘은 데이터의 스케일에 민감하므로, 정규화를 통해 각 feature의 범위를 맞추는 것이 필요하다. 또한, feature와 target을 분리하여 학습 과정에서 목표 변수를 명확히 정의해야 한다.\n",
    "\n",
    "feature은 보통 X에 넣고 target은 y에 넣는게 일반적인 관례이다. 이 표기법은 수학에서 함수를 정의할 때 입력값을 X로, 출력값(목표값)을 y로 표현하는 것에서 유래한 것이다. \n",
    "\n",
    "X: 모델에 입력되는 데이터(특성, feature)를 의미한다.\n",
    "y: 모델이 예측해야 하는 목표값(정답, target)을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4-2. Logistic Regression, Random Forest, XGBoost 모델 학습\n",
    "- **작업:** 세 가지 모델을 사용해 학습하고, 예측 성능을 측정하였다. \n",
    "- **이유:** Logistic regression은 선형 모델로, 데이터의 선형적인 패턴을 학습할 수 있다. Random Forest는 비선형 패턴을 학습하는 데 강점을 가지고 있으며, 다양한 트리를 활용하여 강력한 성능을 발휘한다. XGBoost는 강력한 부스팅 알고리즘으로, 복잡한 패턴을 학습하는 데 효과적이다. 세 가지 모델을 비교함으로써 데이터셋에 가장 적합한 모델을 선택할 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "타이타닉에서 생존자를 예측하는 건 분류모델이다. \n",
    "회귀 모델은 연속적인 수치 값을 예측하는 데 사용된다. 예를 들어, 주택 가격, 온도, 매출액 등과 같은 연속적인 숫자를 예측하는 경우 회귀 모델을 사용한다. \n",
    "\n",
    "반면, 분류 모델은 카테고리(범주형)값을 \n",
    "\n",
    "그래서 타이타닉 생존자를 예측하는 분류 모델이기 때문에\n",
    "분류 모델인 \n",
    "로지스틱 회귀 (Logistic Regresion), 의사결정나무(Decision Tree), 랜덤포레스트(Random Forest), XGBoost등이 사용되었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 대표적인 분류 모델\n",
    "분류 모델은 목표 변수가 이산형(카테고리형)인 경우에 사용된다.\n",
    "\n",
    "자주 사용하는 분류 모델:\n",
    "##### 1. 로지스틱 회귀: 이\n",
    "##### 2. 의사결정 나무(Decision Treee) \n",
    "##### 3. 랜덤 포레스트(Rendom Forest)\n",
    "##### 4. 서포트 벡터 머신(SVM, Support Vector Machine)\n",
    "##### 5. K-최근접 이웃(KNN, K-Nearest Neighbors)\n",
    "##### 6. 나이브 베이즈(Naive Bayse)\n",
    "##### 7. XGBoost: Gradient Boostring 알고리즘을 사용하여 성능이 뛰어난 분류 모델\n",
    "##### 8. Gradient Boosting Classifier\n",
    "##### 9. LightGBM :Gradient Boosting을 개선한 고성능 모델\n",
    "##### 10. 신경망(Neural Network): 심층 신경망(DNN)이나 Convolutional Neural Network(CNN) 등의 딥러닝 기반 모델도 분류 문제에서 많이 사용\n",
    "\n",
    "#### 대표적인 회귀 모델\n",
    "##### 1. 선형 회귀\n",
    "##### 2. 릿지 회귀\n",
    "##### 3. 라쏘 회귀\n",
    "##### 4. ElasticNet 회귀\n",
    "##### 5. 의사결정 나무 회귀\n",
    "##### 6. 랜덤 포레스트 회귀\n",
    "##### 7. 서포트 벡터 회귀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost는 평가 지표로 MSE(평균 제곱 오차)를 사용한다. \n",
    " XGBoost는 Gradient Boosting 알고리즘을 기반으로 한 강력한 모델로, 주로 회귀와 분류 문제에서 좋은 성능을 보입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 이 과제가 **타이타닉 데이터를 가지고 생존자를 예측하는 프로그램을 작성하시오.** 였다면,\n",
    "어떻게 접근해서 풀어야 했을까? \n",
    "\n",
    "튜터님의 가이드에 정답이 있기는 한데, 다시 한번 설명해보자면\n",
    "문제를 처음부터 가이드 없이 접근해야 한다면, **문제 정의 -> 데이터 이해 -> 전처리 -> Feature Engineering -> 모델 학습 -> 평가**의 일련의 과정을 거쳐 문제를 단계적으로 해결해야한다. \n",
    "\n",
    "자꾸 의견이 들어가는데..\n",
    "사실 전처리 기법을 어떻게 작용할지는 데이터 사이언티스트의 판단에 달려있는 거라고 생각한다. \n",
    "\n",
    "#### **문제 해결을 위한 논리적 접근 + 논리를 코드로 풀어내기 + 코드 구현의 이유에 대한 설명력 **\n",
    "\n",
    "결측치를 어떻게 처리했는지\n",
    "스케일링을 어떻게 했는지\n",
    "인코딩을 어떻게 했는지\n",
    "\n",
    "왜 해당 전처리를 선택했는지\n",
    "어떤 feature가 생존 여부에 큰 영향을 미쳤는지\n",
    "왜 로지스틱 회귀를 선택했는지, 다른 모델과 비교한 이유는 무엇인지\n",
    "\n",
    "과제의 핵심은 성능 지표의 비교보다 데이터 이해와 분석 과정에 있을 가능성이 큼\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
