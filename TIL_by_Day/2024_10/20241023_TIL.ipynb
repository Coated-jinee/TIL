{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2024년 12월 23일 수요일\n",
    "## 할일\n",
    "- [x] 프로젝트 기본 과제 코드 작성\n",
    "  - 파이썬 라이브러리 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 팀 프로젝트 기본 과제 코드 작성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 데이터 셋 불러오기\n",
    "   seaborn 라이브러리에 있는 titanic 데이터를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "titanic = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### seaborn library\n",
    "- seaborn은 파이썬에서 데이터 시각화를 위한 라이브러리이다. \n",
    "- matplotlib 기반으로 만들어졌지만, matplotlib보다 더 간결하고 직관적인 코드로 다양한 시각화 작업을 할 수 있게 도와준다.\n",
    "\n",
    "#### 왜 seaborn을 sns라는 약어를 사용해서 불러왔을까? \n",
    "- sns라는 약어는 seaborn 라이브러리를 불러올 때 자주 사용되는 관습적인 약어이다. \n",
    "\n",
    "#### seaborn에 titanic 데이터 셋이 들어가있는 이유는 무엇일까?\n",
    "- seaborn 라이브러리에는 다양한 샘플 데이터셋이 내장되어 있는데, titanic 데이터 셋도 그 중 하나이다.\n",
    "\n",
    "#### 왜 이름이 seaborn일까?\n",
    "- seaborn이라는 이름은 이 라이브러리를 만든 Michael Waskom이 해양과 관련된 테마에서 영감을 받아 지은 이름이다. 구체적으로 seaborn이라는 단어 자체는 '바다(born of the sea)'에서 유래한 것으로, 데이터 시각화에서 데이터를 이해하고 해석하는 과정을 바다를 항해하면서 지도를 그리는 것에 비유한 의미가 담겨있다. \n",
    "\n",
    "#### feature 분석이라는 말의 의미는?\n",
    "- feature는 말 그대로 데이터의 **특징** 또는 **변수**를 의미한다. 머신러닝과 데이터 분석에서 feature는 모델이 학습할 수 있는 **입력 데이터의 속성**들을 말한다. 예를 들어, 타이타닉 데이터셋에서 승객의 나이, 성별, 좌석, 클래스 등은 각각 하나의 feature가 된다.\n",
    "\n",
    "feature분석은 다음과 같은 목적을 가지고 진행된다: \n",
    "1. **데이터의 이해:**각 feature가 무엇을 의미하는지, 그 feature의 분포가 통계적인 특성을 분석하여 데이터셋을 깊이 이해하는 것이 ㅏ목표이다. 이를 통해 분석이나 모델링의 방향을 잡을 수 있다.\n",
    "2. **Feature와 목표 변수 간의 관계 파악:**특정 feature들이 목표 변수(예: 생존 여부, 가격, 클래스 등)와 어떤 관계가 있는지 파악한다. 이를 통해 중요한 feature를 식별하고, 나아가 모델이 더 정확하게 예측할 수 있도록 도움을 준다.\n",
    "3. **중요한 feature 식별:** 모든 feature가 모델링에 동일한 영향을 미치는 것은 아니기 때문에, 중요한 feature와 그렇지 않은 feature를 구분하는 것이 필요하다. 이를 통해 모델의 성능을 개선하거나, feature 수를 줄여 모델을 더 간단하게 만들 수 있다.\n",
    "4. **상관관계 분석:**각 feature간의 상관관계를 분석하여 서로 강하게 관련 된 feature를 찾거나, 불필요하게 중복되는 feature를 제거할 수 있다.\n",
    "\n",
    "즉, feature 분석은 **데이터의 특징들을 이해하고, 그 특징들이 모델의 예측 성능에 미치는 영향을 파악하는 과정이다. 이를 통해 모델이 포함될 주요 변수를 결정하고, 데이터 전처리 및 모델링 전략을 세우는 것이 최종 목표이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. feature 분석\n",
    "   데이터를 잘 불러오셨다면 해당 데이터의 feature를 파악해야합니다. 데이터의 feature를 파악하기 위해 아래의 다양한 feature 분석을 수행해주세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-1. head 함수를 이용해 데이터 프레임의 첫 5행을 출력하여 어떤 feature들이 있는지 확인해주세요. \n",
    "# 2-2. describe 함수를 통해서 기본적인 통계를 확인해주세요. \n",
    "# 2-3. 2-3. describe 함수를 통해 확인할 수 있는 count, std, min, 25%, 50%, 75%, max 가 각각 무슨 뜻인지 주석 혹은 markdown 블록으로 간단히 설명해주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 위 작업을 하는 이유?\n",
    "1. 데이터 프레임의 첫 5행을 확인(head 함수)\n",
    "- 목적: 데이터셋의 전반적인 구조와 각 column이 무엇을 나타내는 지 파악한다. 이를 통해 데이터셋에 어떤 종류의 feature가 포함되어 있는지 확인한다.\n",
    "- 핵심: 각 열이 어떤 정보(변수)를 나타내는 지 확인하고, 분석에 유용한 변수들을 식별함\n",
    "\n",
    "2. 기본적인 통계 확인(describe함수)\n",
    "- 목적: 각 열의 기본 통계값을 계산하여 데이터의 분포를 이해하고, 극단적인 값이나 데이터의 경향성을 파악한다.\n",
    "- 핵심 설명:\n",
    "    - count: 각 열의 유효한 데이터 개수를 의미한다. 이 값이 작으면 해당 열에 결측치가 많다는 것을 알 수 있다.\n",
    "    - mean: 각 열의 평균값으로, 데이터의 중심 경향을 나타낸다.\n",
    "    - std(표준편차)\n",
    "    - min: 각 열의 최소값이다.\n",
    "    - 25%: 1사분위수로, 데이터의 하위 25%에 해당하는 값을 의미한다.\n",
    "    - 50%: 중앙값(2사분위수)으로, 데이터의 중간값을 나타낸다.\n",
    "    - 75%: 3사분위수로, 데이터의 상위 25%에 해당하는 값을 의미한다.\n",
    "    - max: 각 열의 최대값이다.\n",
    "- 핵심: 각 feature의 분포를 확인하고, 이상치나 데이터의 특성을 파악하는 데 도움을 준다.\n",
    "\n",
    "3. 결측치 확인(isnull() + sum())\n",
    "- 목적: 각 열에 존재하는 결측치의 개수를 파악한다. 결측치가 많은 열은 분석 과정에서 처리해야 할 필요가 있다.\n",
    "- 핵심: 결측치가 많은 열이나 분석에 중요한 열의 결측치를 식별하여, 데이터 전처리 전략을 세울 수 있다. \n",
    "\n",
    "결론적으로, 이러한 단계들은 데이터셋을 깊이 이해하고, 분석 과정에서 중요한 feature를 식별하며, 데이터의 품질을 파악하여 향후 모델링에 유리한 방향을 결정하는 데 중요한 역할을 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. feature engineering**\n",
    "feature engineering은 모델의 성능을 향상시키기 위해 중요한 단계입니다. 2번 feature 분석에서 얻은 데이터에 대한 이해를 바탕으로 아래의 feature engineering을 수행해주세요. \n",
    "\n",
    "**3-1. 결측치 처리**\n",
    "Age(나이)의 결측치는 중앙값으로, Embarked(승선 항구)의 결측치는 최빈값으로 대체해주세요. \n",
    "모두 대체한 후에, 대체 결과를 isnull() 함수와 sum()  함수를 이용해서 확인해주세요. \n",
    "\n",
    "**3-2. 수치형으로 인코딩**\n",
    "Sex(성별)를 남자는 0, 여자는 1로 변환해주세요. alive(생존여부)를 yes는 1, no는 0으로 변환해주세요. \n",
    "Embarked(승선 항구)는 ‘C’는 0으로, Q는 1으로, ‘S’는 2로 변환해주세요. 모두 변환한 후에, 변환 결과를 head 함수를 이용해 확인해주세요. \n",
    "\n",
    "**3-3. 새로운 feature 생성**\n",
    "SibSip(타이타닉호에 동승한 자매 및 배우자의 수), Parch(타이타닉호에 동승한 부모 및 자식의 수)를 통해서 family_size(가족크기)를 생성해주세요. 새로운 feature를 head 함수를 이용해 확인해주세요. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 위 작업을 하는 이유?\n",
    "\n",
    "1. **결측치 처리**\n",
    "- 작업: 결측치를 중앙값(Age)과 최빈값(Enbarked)으로 대체\n",
    "- 이유: 결측치는 모델의 성능을 떨어트리고 오류를 유발할 수 있다. Age의 경우 중앙값으로 대체하는 이유는 분포가 크게 치우치지 않도록 하기 위함이고, Embarked는 범주형 데이터이므로 가장 많이 등장한 값으로 대체하는 것이 합리적이다. 이렇게 하면 데이터의 일관성을 유지하면서, 모델링 과정에서 결측치로 인한 오류를 방지할 수 있다.\n",
    "\n",
    "2. **수치형으로 인코딩**\n",
    "- 작업: 성별과 생존 여부, 승선 항구 정보를 수치형 데이터로 변환\n",
    "- 이유: 머신러닝 모델은 숫자를 입력으로 받아야 하기 때문에, 범주형 데이터를 수치형으로 변환해야한다. 이 작업을 통해 데이터셋을 모델에 입력할 수 있는 형식으로 준비하고, 모델이 범주형 변수의 정보를 학습할 수 있도록 함\n",
    "\n",
    "3. **새로운 feature 생성**\n",
    "- 작업: SibSp와 Parch를 합하여 family_size라는 새로운 feature를 생성\n",
    "- 이유: 새로운 feature를 생성함으로써 모델이 승객의 가족 크기라는 중요한 정보를 학습할 수 있게 된다. 이런 추가적인 정보는 모델의 성능을 향상시킬 수 있다.\n",
    "\n",
    "4. **모델 학습 준비 및 학습**\n",
    "- 작업: 학습에 필요한 feature를 선택하고, target 변수를 분리한 후, 스케일링을 진행\n",
    "- 이유: 데이터 스케일링은 모델 학습에서 중요한 단계이다. 특히, Logistic Regression과 XGboost와 같은 알고리즘은 데이터의 스케일에 민감하므로, 정규화를 통해 각 feature으 범위를 맞추는 것이 필요하다. 또한, feature와 target을 분리하여 학습 과정에서 목표 변수를 명확하게 정의해야 한다.\n",
    "\n",
    "5. **Logistic Regression, Random Forest, XGBoost 모델 학습**\n",
    "- 작업: 세 가지 모델을 사용해 학습하고, 예측 성능을 측정함\n",
    "- 이유: Logistic Regression은 선형 모델로, 데이터의 선형적인 패턴을 학습할 수 있다. Random Forest는 비선형 패턴을 학습하는 데 강점을 가지고 있으며, 다양한 트리를 활용하여 강력한 성능을 발휘한다. XGBoost는 강력한 부스팅 알고리즘으로, 복잡한 패턴을 학습하는 데 효과적이다. 세 가지 모델을 비교함으로써 데이터셋에 가장 적합한 모델을 선택할 수 있다. \n",
    "\n",
    "이러한 단계들은 데이터 전처리, 변환, 학습, 평가까지의 과정을 통해 최적의 모델을 구축하고, 데이터로부터 정확한 예측을 얻기 위해 수행된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 만약 문제가 '타이타닉데이터를 가지고 생존자를 예측하는 프로그램을 작성하시오.'만 주어졌다면, 어떤식으로 문제에 접근해야 했을까?\n",
    "- 문제를 단계적으로 접근하는 것이 중요하다. 가이드가 없더라도 논리적인 문제 해결 과정을 따르면 혼란을 줄일 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **문제 이해 및 목표 정의**\n",
    "- 목표: 타이타닉 데이터셋을 이용하여 승객이 생존할지 여부를 예측하는 프로그램을 작성해야 함\n",
    "- 타겟 변수: survived (생존 여부를 나타내는 0 또는 1 값)\n",
    "- 예측 변수: 타겟 변수에 영향을 미치는 변수들 (예: 성별, 나이, 객실 등급 등)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **데이터 탐색 (EDA, Exploratory Data Analysis)**\n",
    "- **데이터 확인:** 데이터셋의 첫 5행을 확인 (head()함수 사용)\n",
    "- **데이터 구조 파악:** 각 feature의 데이터 타입, 결측치 여부, 기초 통계량 확인 (info(), describe(), isnull() .sum())\n",
    "- **데이터 분포 확인:** 수치형 feature의 분포 확인 및 target 변수와의 관계 파악 (히스토그램, 박스플롯, 카운드플롯 등 시각화)\n",
    "\n",
    "    **이유:** EDA를 통해 데이터의 기본 구조를 이해하고, 문제 해결의 방향성을 설정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **데이터 전처리 (Data Preprocessing)**\n",
    "- **결측치 처리:** 결측치를 중앙값이나 최빈값 등으로 대체하거나, 분석에 중요한 변수의 경우 결측치 행 제거\n",
    "- **범주형 변수 인코딩:** 모델이 범주형 변수를 이해할 수 있도록 수치형으로 인코딩 (예: sex, embarked)\n",
    "- **필요한 feature 선택:** 데이터 분석 결과를 바탕으로 학습에 필요한 feature를 선택\n",
    "    **이유:** 데이터 전처리를 통해 모델의 성능에 악영향을 미칠 수 있는 결측치, 비정상적인 값 등을 처리하여 모델 학습의 기초를 튼튼히 함 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Feature Engineering**\n",
    "- **새로운 feature 생성**: 기존 featuref를 조합하여 새로운 feature를 만든다 (예: family_size)\n",
    "- **Feature 중요도 평가**: target 변수와의 상관관계를 바탕으로 중요 feature를 식별한다.\n",
    "\n",
    "    **이유:** 좋은 feature는 모델의 예측 성능을 크게 향상시킬 수 있다. 데이터에 대해 충분히 이해하고, 추가적인 정보를 제공할 수 있는 feature를 생성한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **데이터 분할 및 스케일링**\n",
    "- **데이터 분할**: 학습용 데이터와 테스트용 데이터로 분할 (예: train_test_split)\n",
    "- **스케일링**: 모델의 성능을 높이기 위해 수치형 데이터의 크기를 정규화 (StrandardScaler 사용)\n",
    "    **이유**: 데이터의 스케일을 조정하면 모델의 성능이 향상될 수 있으며, 과적합을 방지할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **모델 선택 및 학습 (Model Selection and Training)**\n",
    "- **여러 모델 선택**: 선형 모델, 트리 기반 모델, 부스팅 모델 등 다양한 알고리즘을 선택 (예: Logistic Regression, Random Forest, XGBoost)\n",
    "- **모델 학습**: 선택한 모델을 사용해 학습 데이터를 통해 학습\n",
    "- **모델 평가**: 예측 성능을 평가하고, 적절한 지표를 통해 성능을 비교 (정확도, MSE, ROC-AUC 등)\n",
    "    **이유:** 다양한 모델을 사용하여 성능을 비교하고, 데이터셋에 가장 적합한 모델을 선택함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 모델 평가 및 하이퍼파라미터 듀닝\n",
    "- **교차 검증**: 모델의 성능을 더 정확하게 평가하기 위해 교차 검증을 수행\n",
    "- **하이퍼파라미터 튜닝**: GridSearchCV 등 방법을 통해 모델의 성능을 최적화\n",
    "\n",
    "    **이유:**: 모델의 성능을 더욱 향상시키기 위해 다양한 파라미터 설정을 시도하고, 최적의 조합을 찾음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 결과 분석 및 인사이트 도출\n",
    "- **모델 해석**: 중요한 feature가 무엇인지, 어떤 feature가 생존에 가장 큰 영향을 미쳤는지 분석\n",
    "- **결과 시각화**: 모델의 예측 결과를 시각화하고, 실제 결과와 비교하여 성능을 이해한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**요약**\n",
    "\n",
    "문제를 처음부터 가이드 없이 접근해야 한다면, 문제 정의 → 데이터 이해 → 전처리 → Feature Engineering → 모델 학습 → 평가의 일련의 과정을 거쳐 문제를 단계적으로 해결하기\n",
    "\n",
    "각 단계에서 목표와 이유를 명확히 하며, 이를 바탕으로 코드를 작성하고 분석을 진행하는 것이 중요\n",
    "\n",
    "이를 통해, 문제의 해결뿐만 아니라, 데이터의 이해와 인사이트 도출, 모델의 성능 평가와 최적화까지 포괄적으로 고려할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다양한 데이터 전처리 기법\n",
    "\n",
    "1. 결측치 처리 (Missing Value Handing)\n",
    "- 기법: 결측치 제거, 평균/중앙값/최빈값 대체, 예측을 통한 대체\n",
    "\n",
    "2. 이상치 처리 (Outlier Handling)\n",
    "- 기법: IQR(Interquartile Range) 방법, 표준편차 기반, 로그 변환, 윈저화(Winsorization)\n",
    "- 목적: 데이터 내 이상치를 처리함으로써 모델이 이상치의 영향을 덜 받도록 하고, 예측의 정확도를 높임\n",
    "\n",
    "3. 데이터 정규화 및 표준화 (Normalization & Strandardization)\n",
    "- 기법: Min-Max Scaling, Z-score Scaling (StrandardScaler)\n",
    "- 목적: 각 feature의 스케일을 동일하게 맞춰줌으로써, 모델이 특정 feature의 크기 차이로 인해 영향을 받지 않도록 함. 특히, 거리 기반 알고리즘(KNN, SVM 등)에서는 필수적이다.\n",
    "\n",
    "4. 범주형 데이터 인코딩\n",
    "- 기법: One-hot Encoding, Lable Encoding, Target Encoding "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
