{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# 2024년 11월 15일 금요일\n",
    "## 할일\n",
    "- [x] 개인과제확인\n",
    "- [x] 특강: LLM 특강\n",
    "- [x] 특강: API 특강\n",
    "- [x] 베이직반: Prompt Engineering 3회차 수업\n",
    "\n",
    "### 오전 특강: LLM 특강 1강\n",
    "RNN 으로 부터 출발해서 LLM으로 도달하게 된 과정\n",
    "\n",
    "LLM..\n",
    "DNN (입력층 출력층 사이에 히든레이어 있음)\n",
    "\n",
    "기존에 딥러닝으로 언어 모델을 만들때 쓰이던 RNN은 몇가지 한계점이 있음\n",
    "RNN은 이전 단계의 정보를 다음 단계에서 활용하는데, RNN 모델이 커지면서 이 부분에 문제가 생김\n",
    "기울기 소실 문제 (Vanishing Gradient Problem)가 대표적임. -> 뒷부분이 흐려진다.\n",
    "그래서 LSTM과 GRU등의 방법이 고안되었지만, 정보가 매 단계로 순차적으로 전달되는 RNN의 구조적인 한계로 완전히 극복할 수 없었음\n",
    "\n",
    "Attention이라는 기법이 RNN에 도입됨 \n",
    "매 단계에서 Attention을 계산해서 어떤 이전 단계에 집중해서 \n",
    "이때까지는 RNN의 기본 구조를 사용하면서 Attention을 활용하는 식이였음\n",
    "\n",
    "근데 구글에서 2017년에 논문을 발표. RNN을 빼버리고 Attention만 이용해서 모델을 만들었는데 이게 RNN + Attention 보다 뛰어나더라 이게 Transformer 모델임 \n",
    "\n",
    "RNN은 순차적으로 계산을 해야했음\n",
    "트랜스포머는 Attention 만을 사용했고, 이는 단계를 필요로 하지 않음\n",
    "Attention, 즉 각 단어가 다른 단어와 얼마나 관계가 있는지의 점수는 단계가 필요없이 병렬적으로 계산됨\n",
    "(GPU가 병렬적 동시계산을 빠르게 해줘서 가능해짐)\n",
    "학습속도가 비약적으로 빨라졌음. Attention을 다양한 각도에서 여러번 계산하는 것도 가능해지면서 성능도 비약적으로 \n",
    "\n",
    "RNN과 달리 트랜스포머는 Attention을 사용해서 장기의존성(Long-Term Dependencies)을 학습하기도 쉬워짐\n",
    "모델에 더 깊은 레이어를 만들 수 있고, 더 빨리 학습하고 정확도도 들었다 즉 언어 모델이 크기의 limit을 해제 했다는 뜻 \n",
    "이때부터 수천억개의 파라미터를 가진 대형 모델이 나오게 됨\n",
    "\n",
    "———\n",
    "\n",
    "LLM은 어떤식으로 배우게 되나? \n",
    "트랜스포머 구조 때문에 압도적인 확장이 가능해졌음\n",
    "\n",
    "GPT4o은 몇백억개의 파라미터를 가졌다고 추정됨\n",
    "가중치가 Float 32bit 형태로 계산되게 되는데..\n",
    "대충 계산해도 몇백GB의 크기 (메모리에 올라가야 하는 용량 - RAM )\n",
    "\n",
    "16GB RAM, 32GB RAM, 32GB RAM .. 다램익선..\n",
    "\n",
    "최소 몇백GB 램이 필요하다고 생각하면 됨 \n",
    "- 근데 이걸 VRAM에 올려야함 (4090이 VRAM 24GB를 확보하려면 약 300만원)\n",
    "- SK 하이닉스 HBM, 삼성전자가 HBM 경쟁력을 잃었다.\n",
    "\n",
    "일반적인 컴퓨터로는 학습, 실행이 당연히 불가능함.\n",
    "\n",
    "지금까지의 딥러닝 실습과는 달리 직접 LLM 모델을 만들고 학습하는 걸 배우지 못하는 이유임\n",
    "그래서 보통 큰 기업에서 만들어서 제공하는 모델들을 사용하게 됨\n",
    "OpenAI의 GPT, Google의 Gemini 등등이 있음\n",
    "이러한 모델을 렌트해서 쓰는 대신 렌트비를 내게 됨\n",
    "\n",
    "LLM은 제공되는 모델을 활용하는 방법으로 학습하게 될 것\n",
    "빠르게 업데이트 되고 있기 때문에 몇 달 전에 올라온 자료가 틀릴 수도 있음\n",
    "공식문서와 가이드를 읽고 활용하는 방법을 배우기\n",
    "\n",
    "———\n",
    "LLM 활용의 기본적인 컨셉\n",
    "LLM은 Text Completion에 특히 강점을 가지고있음\n",
    "문맥 (Context)을 읽고 다음에 올 단어를 예측하여 문장을 완성하는 것\n",
    "“나는 블루베리 ____”를 입력하면 “나는 블루베리 치즈케이크를 좋아해”로 완성해줄 수 있는 것임\n",
    "Context로 줄 수 있는 토큰의 사이즈가 커지니 다양한 일들이 가능해짐\n",
    "입력이 아래와 같이 주어지면 (너는 친절한 AI 보조원이야. 사용자의 질문에 상세하게 답변하도록 해)\n",
    "\n",
    "ChatGPT에 \n",
    "\n",
    "LLM에 입력으로 주어지는 텍스트 데이터를 프롬프트(Prompt)라고 부름\n",
    "prompt는 원래 단어로 “촉발하다”라는 뜻을 가짐\n",
    "\n",
    "LLM에게 마중물 처럼 주어지는 텍스트가 컴퓨터 프로그래밍에서 주로 사용되던 ..\n",
    "\n",
    "LLM의 기본적인 활용은 원하는 답변을 얻을 수 있는 효과적인 프롬프트를 만들어내는걸 목표로함\n",
    "일반적으로 사용자의 대화 바깥에 위치한 맥락인 System Message와 사용자와 챗봇간의 대화 내용인 User Message, Assistant Message 로 구분됨\n",
    "\n",
    "요즘의 LLM은 긴 길이의 프롬프트도 잘 처리할 수 있긴 하지만, 프롬프트가 무작정 길어지는 것도 좋지 않음\n",
    "프롬프트가 길어질수록 답변이 부정확해지고, 실행 시간도 길어짐. 비용도 더 많이 들어감.\n",
    "\n",
    "일반적으로 프롬프트의 길이를 세는 단위는 Token임\n",
    "Token은 AI 모델이 한번에 이해할 수 있는 크기를 말함\n",
    "\n",
    "\n",
    "GPT-4o는 100만토큰에 2.5달러 (input)\n",
    "output은 10달러 \n",
    "100만글자의 입력은 3500원, 100만단어가 나오게 되면 14000원\n",
    "\n",
    "책 다섯권을 통째로 넣으면 3500원 정도 나옴.\n",
    "응답으로 책 다섯권을 쓸 수 있는 비용이 14000원 (한국어 기준)\n",
    "\n",
    "영어는 좀 더 저렴해짐\n",
    "\n",
    "돈이있다고 무작정 Token을 늘릴 수 있는 것도 아님\n",
    "LLM이 아무리 크다고 해도 한계가 있음\n",
    "LLM이 하나의 맥락으로 인식할 수 있는 범위를 Context window 라고 함\n",
    "모델에 따라서 한 번의 출력에 나올 수 있는 토큰 개수가 ...\n",
    "—— \n",
    "\n",
    "LLM을 가져다 써보기\n",
    "가장 대표적으로 많이 사용되는 OpenAI의 API를 사용해볼 것\n",
    "\n",
    "——\n",
    "Colab으로 실습\n",
    "\n",
    "조별로 api 키가 제공되었음 . \n",
    "보통은 api 키를 코드에 넣지 않으려고 하고, 환경 변수에 넣거나 함\n",
    "코랩에서 관리하는 환경변수 관리법\n",
    "\n",
    "나머지는 다음 특강에 이어진다.\n",
    "\n",
    "### 베이직반: Prompt Engineering 3회차\n",
    "- 결석 때문에 이 3회차 분량의 수업 중 거의 마지막 수업만 참여하게 됐는데, 튜터님이 그동안의 내용을 짚어주시고 실습에 들어가주신 덕에 3회차 수업만 들었지만 대략적인 흐름 파악과 실습 따라가기가 가능했고, 코드와 문서만 보고도 이해가 될 정도로 문서화를 잘해주셨다. \n",
    "그리고 실습이 colab으로 진행되었는데, 정말 운 좋게도 colab 환경변수에 api 키 넣는법을 오전에 배웠다. 완전 럭키비키자너;\n",
    "RAG가 아직 어떤 개념인지는 모르겠는데, 왠지 api를 써서 직접 정보를 넣어주고 답변 받고 이런느낌. 이런 기술인데 이 안에 기술적으로 질문 답변을 잘 받기 위해 프롬프트 엔지니어링이 들어가는 것 같은 느낌\n",
    "\n",
    "### 오후 특강: API 특강\n",
    "- 네이버 API를 가지고 실습 예시를 보여주시고, 예제 코드가 제공되었다. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
